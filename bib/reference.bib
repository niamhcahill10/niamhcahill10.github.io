@inproceedings{Tercero2025Vibrotactile,
  abstract = {This paper explores the use of a body-worn vibrotactile vest to convey real-time information from robot to operator. Vibrotactile communication could be useful in providing information without compromising or loading a person's visual or auditory perception. This paper considers applications in Urban Search and Rescue (USAR) scenarios where a human working alongside a robot is likely to be operating in high cognitive load conditions. The focus is on understanding how best to convey information considering different vibrotactile information coding strategies to enhance scene understanding in scenarios where a robot might be operating remotely as a scout. In exploring information representation, this paper introduces the idea of Semantic Haptics, using shapes and patterns to represent certain events as if the skin was a screen, and shows how these lead to better learnability and interpretation accuracy.},
  address = {Singapore},
  author = {Tercero, Adrian Vecina and Caleb-Solly, Praminda},
  booktitle = {Social Robotics},
  editor = {Palinko, Oskar and Bodenhagen, Leon and Cabibihan, John-John and Fischer, Kerstin and {\v{S}}abanovi{\'{c} Selma and Winkle, Katie and Behera, Laxmidhar and Ge, Shuzhi Sam and Chrysostomou, Dimitrios and Jiang, Wanyue and He, Hongsheng},
  isbn = {978-981-96-3525-2},
  keywords = {type:system, Human-Robot Collaboration, Vibrotactile communication},
  pages = {467--478},
  publisher = {Springer Nature Singapore},
  series = {Lecture Notes in Computer Science},
  title = {Vibrotactile Information Coding Strategies for a Body-Worn Vest to Aid Robot-Human Collaboration},
  year = {2025},
  doi = {https://doi.org/10.1007/978-981-96-3525-2_39}
}


@inproceedings{Benford2025Somatic,
  abstract = {As robots enter the messy human world so the vital matter of safety takes on a fresh complexion with physical contact becoming inevitable and even desirable. We report on an artistic-exploration of how dancers, working as part of a multidisciplinary team, engaged in contact improvisation exer- cises to explore the opportunities and challenges of dancing with cobots. We reveal how they employed their honed bodily senses and physical skills to engage with the robots aesthetically and yet safely, interleaving improvised physical manipulations with reflections to grow their knowledge of how the robots behaved and felt. We introduce somatic safety, a holistic mind-body approach in which safety is learned, felt and enacted through bodily contact with robots in addition to being reasoned about. We conclude that robots need to be better designed for people to hold them and might recognise tacit safety cues among people.We propose that safety should be learned through iterative bodily experience interleaved with reflection.},
  author = {Benford, Steve and Schneiders, Eike and Martinez Avila, Juan Pablo and Caleb-Solly, Praminda and Brundell, Patrick Robert and Castle-Green, Simon and Zhou, Feng and Garrett, Rachael and H\"{o}\"{o}k, Kristina and Whatley, Sarah and Marsh, Kate and Tennent, Paul},
  booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
  url = {https://dl.acm.org/doi/10.5555/3721488.3721543},
  keywords = {type:system, dance, human-robot interaction, robotics, safety, soma design, somatic safety},
  location = {Melbourne, Australia},
  numpages = {10},
  pages = {429–438},
  publisher = {IEEE Press},
  series = {ACM/IEEE HRI},
  title = {Somatic Safety: An Embodied Approach Towards Safe Human-Robot Interaction},
  year = {2025}
}

@article{Casalino2018OperatorAwareness,
  abstract = {In industrial scenarios, requiring human–robot collaboration, the understanding between the human operator and his/her robot coworker is paramount. On the one side, the robot has to detect human intentions, and on the other side, the human needs to be aware of what is happening during the collaborative task. In this letter, we address the first issue by predicting human behavior through a new recursive Bayesian classifier, exploiting head, and hand tracking data. Human awareness is tackled by endowing the human with a vibrotactile ring that sends acknowledgments to the user during critical phases of the collaborative task. The proposed solution has been assessed in a human–robot collaboration scenario, and we found that adding haptic feedback is particularly helpful to improve the performance when the human–robot cooperation task is performed by nonskilled subjects. We believe that predicting operator's intention and equipping him/her with wearable interface, able to give information about the prediction reliability, are essential features to improve performance in a human–robot collaboration in industrial environments.},
  author = {Casalino, Andrea and Messeri, Costanza and Pozzi, Maria and Zanchettin, Andrea Maria and Rocco, Paolo and Prattichizzo, Domenico},
  doi = {10.1109/LRA.2018.2865034},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {type:system, Collaboration, Haptic interfaces, Task analysis, Robot sensing systems, Trajectory;Service robots, Human-centered robotics, haptics and haptic interfaces, cognitive human-robot interaction},
  number = {4},
  pages = {4289-4296},
  series = {IEEE RAL},
  title = {Operator Awareness in Human–Robot Collaboration Through Wearable Vibrotactile Feedback},
  volume = {3},
  year = {2018}
}

@inproceedings{Cha2018Auditory,
  abstract = {Auditory cues facilitate situational awareness by enabling humans to infer what is happening in the nearby environment. Unlike humans, many robots do not continuously produce perceivable state-expressive sounds. In this work, we propose the use of iconic auditory signals that mimic the sounds produced by a robot»s operations. In contrast to artificial sounds (e.g., beeps and whistles), these signals are primarily functional, providing information about the robot»s actions and state. We analyze the effects of two variations of robot sound, tonal and broadband, on auditory localization during a human-robot collaboration task. Results from 24 participants show that both signals significantly improve auditory localization, but the broadband variation is preferred by participants. We then present a computational formulation for auditory signaling and apply it to the problem of auditory localization using a human-subjects data collection with 18 participants to learn optimal signaling policies.},
  address = {New York, NY, USA},
  author = {Cha, Elizabeth and Fitter, Naomi T. and Kim, Yunkyung and Fong, Terrence and Matari\'{c Maja J.},
  booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  doi = {10.1145/3171221.3171285},
  isbn = {9781450349536},
  keywords = {type:system, sound, nonverbal communication, human-robot interaction, coordination, collaboration, auditory localization},
  location = {Chicago, IL, USA},
  numpages = {9},
  pages = {434–442},
  publisher = {Association for Computing Machinery},
  series = {ACM/IEEE HRI},
  title = {Effects of Robot Sound on Auditory Localization in Human-Robot Collaboration},
  url = {https://doi.org/10.1145/3171221.3171285},
  year = {2018}
}

@article{Christie2024LIMITLI,
  abstract = {Robots can use auditory, visual, or haptic interfaces to convey information to human users. The way these interfaces select signals is typically pre-defined by the designer: for instance, a haptic wristband might vibrate when the robot is moving and squeeze when the robot stops. But different people interpret the same signals in different ways, so that what makes sense to one person might be confusing or unintuitive to another. In this article, we introduce a unified algorithmic formalism for learning co-adaptive interfaces from scratch. Our method does not need to know the human’s task (i.e., what the human is using these signals for). Instead, our insight is that interpretable interfaces should select signals that maximize correlation between the human’s actions and the information the interface is trying to convey. Applying this insight we develop Learning Interfaces to Maximize Information Transfer (LIMIT). LIMIT optimizes a tractable, real-time proxy of information gain in continuous spaces. The first time a person works with our system the signals may appear random; but over repeated interactions, the interface learns a one-to-one mapping between displayed signals and human responses. Our resulting approach is both personalized to the current user and not tied to any specific interface modality. We compare LIMIT to state-of-the-art baselines across controlled simulations, an online survey, and an in-person user study with auditory, visual, and haptic interfaces. Overall, our results suggest that LIMIT learns interfaces that enable users to complete the task more quickly and efficiently, and users subjectively prefer LIMIT to the alternatives. See videos here: .},
  address = {New York, NY, USA},
  articleno = {56},
  author = {Christie, Benjamin A. and Losey, Dylan P.},
  doi = {10.1145/3675758},
  issue_date = {December 2024},
  journal = {J. Hum.-Robot Interact.},
  keywords = {type:system, Interfaces, Information Theory, Co-Adaption, Human-Robot Interaction},
  month = {oct},
  number = {4},
  numpages = {26},
  publisher = {Association for Computing Machinery},
  title = {LIMIT: Learning Interfaces to Maximize Information Transfer},
  url = {https://doi.org/10.1145/3675758},
  volume = {13},
  year = {2024},
  series = {ACM THRI}
}

@article{Habibian2024Survey,
  abstract = {For robots to seamlessly interact with humans, we first need to make sure that humans and robots understand one another. Diverse algorithms have been developed to enable robots to learn from humans (i.e., transferring information from humans to robots). In parallel, visual, haptic, and auditory communication interfaces have been designed to convey the robot’s internal state to the human (i.e., transferring information from robots to humans). Prior research often separates these two directions of information transfer, and focuses primarily on either learning algorithms or communication interfaces. By contrast, in this survey we take an interdisciplinary approach to identify common themes and emerging trends that close the loop between learning and communication. Specifically, we survey state-of-the-art methods and outcomes for communicating a robot’s learning back to the human teacher during human-robot interaction. This discussion connects human-in-the-loop learning methods and explainable robot learning with multimodal feedback systems and measures of human-robot interaction. We find that—when learning and communication are developed together—the resulting closed-loop system can lead to improved human teaching, increased human trust, and human-robot co-adaptation. The paper includes a perspective on several of the interdisciplinary research themes and open questions that could advance how future robots communicate their learning to everyday operators. Finally, we implement a selection of the reviewed methods in a case study where participants kinesthetically teach a robot arm. This case study documents and tests an integrated approach for learning in ways that can be communicated, conveying this learning across multimodal interfaces, and measuring the resulting changes in human and robot behavior.},
  author = {Soheil Habibian and Antonio Alvarez Valdivia and Laura H. Blumenschein and Dylan P. Losey},
  doi = {10.1177/02783649241281369},
  eprint = {https://doi.org/10.1177/02783649241281369 ​ ,},
  journal = {The International Journal of Robotics Research},
  keywords = {type:system, Human-Robot Interaction, Robot Learning, Virtual Reality and Interfaces},
  number = {4},
  pages = {665-698},
  series = {IJRR},
  title = {A survey of communicating robot learning during human-robot interaction},
  url = {https://doi.org/10.1177/02783649241281369 ​ ,},
  volume = {44},
  year = {2024}
}

@inproceedings{Kosuge2003MsDanceR,
  abstract = {We propose a dance partner robot referred to as Ms DanceR (Mobile Smart Dance Robot), which has been developed as platform for realizing the effective human-robot coordination with physical interaction. Ms DanceR consists of an omni-directional mobile base and a Body Force Sensor, which is a force/torque sensor installed between the mobile base and the body of the robot. A human could dance a ballroom dance together with Ms DanceR based on a control architecture referred to as "CAST" (Control Architecture based-on Step Transition), which was designed according to features of ballroom dances.},
  author = {Kosuge, K. and Hayashi, T. and Hirata, Y. and Tobiyama, R.},
  booktitle = {Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
  doi = {10.1109/IROS.2003.1249691},
  keywords = {type:system, Robot kinematics, Human robot interaction, Robot sensing systems, Intelligent robots, Mobile robots, Positron emission tomography;Torque;Medical robotics;Motion control;Seals},
  pages = {3459-3464 vol.3},
  series = {IEEE/RSJ IRS},
  title = {Dance partner robot - Ms DanceR},
  volume = {4},
  year = {2003}
}

@inproceedings{Manoj2014HapiMoto,
  abstract = {A national study by the Australian Transport Safety Bureau revealed that motorcyclist deaths were nearly thirty times more prevalent than that of drivers of other vehicles. These fatalities represent approximately 5\% of all highway deaths each year, yet motorcycles account for only 2\% of all registered vehicles in the USA. Motorcyclists are highly exposed on the road, so maintaining situational awareness at all times is crucial. Route guidance systems enable users to efficiently navigate between locations using dynamic visual maps and audio directions, and have been well tested with motorists, but remain unsafe for use by motorcyclists. Audio/visual routing systems decrease motorcyclists' situational awareness and vehicle control, and thus elevate chances of an accident. To enable motorcyclists to take advantage of route guidance while maintaining situational awareness, we created HaptiMoto, a wearable haptic route guidance system. HaptiMoto uses tactile signals to encode the distance and direction of approaching turns, thus avoiding interference with audio/visual awareness. Our evaluations demonstrate that HaptiMoto is both intuitive and a safer alternative for motorcyclists compared to existing solutions.},
  address = {New York, NY, USA},
  author = {Prasad, Manoj and Taele, Paul and Goldberg, Daniel and Hammond, Tracy A.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  doi = {10.1145/2556288.2557404},
  isbn = {9781450324731},
  keywords = {type:system, vibro-tactile, tactile interface, route guidance, advanced traveler information system},
  location = {Toronto, Ontario, Canada},
  numpages = {10},
  pages = {3597–3606},
  publisher = {Association for Computing Machinery},
  series = {SIGCHI HFCS},
  title = {HaptiMoto: turn-by-turn haptic route guidance interface for motorcyclists},
  url = {https://doi.org/10.1145/2556288.2557404},
  year = {2014}
}

@inproceedings{Toscano2022SARs,
  abstract = {Despite the growing interest in smart homes and robotics in many domains, very few studies have explored how socially assistive robots (SAR) can be integrated into smart homes to control them while socially interacting with people. This paper explores two factors - embodiment and movement - that influence the human-robot interaction into a domestic context. We conducted a within-subjects study with three con-ditions (disembodied-static, embodied-static, embodied-dynamic) involving the conventional population. Participants (N},
  author = {Toscano, Eleonora and Spitale, Micol and Garzotto, Franca},
  booktitle = {2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  doi = {10.1109/HRI53351.2022.9889467},
  keywords = {type:system, Temperature, Ovens, Sociology, Smart homes, Assistive robots, Temperature control, Task analysis, socially assistive robots, smart home, embodiment, movement, social presence, perceived sociability},
  pages = {1075-1079},
  series = {ACM/IEEE HRI},
  title = {Socially Assistive Robots in Smart Homes: Design Factors that Influence the User Perception},
  year = {2022}
}

@article{Triantafyllidis2020Multimodal,
  abstract = {Research in multimodal interfaces aims to provide immersive solutions and to increase overall human performance. A promising direction is to combine auditory, visual and haptic interaction between the user and the simulated environment. However, no extensive comparison exists to show how combining audiovisuohaptic interfaces would affect human perception and by extent reflected on task performance. Our paper explores this idea and presents a thorough, full-factorial comparison of how all combinations of audio, visual and haptic interfaces affect performance during manipulation. We evaluated how each combination affects the performance in a study (N},
  author = {Triantafyllidis, Eleftherios and Mcgreavy, Christopher and Gu, Jiacheng and Li, Zhibin},
  doi = {10.1109/ACCESS.2020.2990080},
  journal = {IEEE Access},
  keywords = {type:system, Task analysis, Visualization, Haptic interfaces, Robot sensing systems, Robot kinematics, Virtual reality, Audiovisuohaptic, auditory feedback, haptic feedback, immersive manipulation, immersive teleoperation, multimodal interaction, multimodal interface, virtual reality},
  pages = {78213-78227},
  series = {IEEE Access},
  title = {Study of Multimodal Interfaces and the Improvements on Teleoperation},
  volume = {8},
  year = {2020}
}

@article{Unbehaun2025Rehab,
  abstract = {This paper explores technical implications and design opportunities that are conceptualized to inform a socio-robotic system with digital applications to support the recovery process of patients within a rehabilitation facility. By conducting observations and interviews with patients and therapists, we identified key challenges and design opportunities in a specific orthopaedic rehabilitation context and process. The findings indicate the design potentials of a socio-robotic system to enhance patient engagement and recovery by providing personalized activities, a meaningful interaction and a motivating surrounding by using music-based exercises. Our research suggests that integrating digital applications with robotic systems may be used in the long-run to offer tailored exercises, stimulating concepts to motivate and maintain patients in therapy process, real-time feedback, and data-driven progress tracking, thereby improving the overall therapeutic outcomes. By addressing these factors, our proposed socio-robotic system aims to create a more interactive and engaging orthopaedic rehabilitation experience and environment, ultimately supporting patient recovery and improving overall treatment.},
  author = {David Unbehaun and Marcel Kirsch and Johannes Schulte-Huermann and Julia Beckmann and Dominik Schulz and Katharina Thiel and Thomas Fritz and Volker Wulf},
  doi = {doi:10.1515/icom-2024-0044},
  journal = {i-com},
  keywords = {type:system, socio-robotic systems, socio informatics, participatory design, music, rehabilitation, design implications},
  lastchecked = {2025-05-13},
  number = {1},
  pages = {193--211},
  series = {i-com},
  title = {Exploring technical implications and design opportunities for interactive and engaging telepresence robots in rehabilitation – results from an ethnographic requirement analysis with patients and health-care professionals},
  url = {https://doi.org/10.1515/icom-2024-0044},
  volume = {24},
  year = {2025}
}

